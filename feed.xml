<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://zj-gao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zj-gao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-05T00:03:11+00:00</updated><id>https://zj-gao.github.io/feed.xml</id><title type="html">ZJ Gao’s Research Site</title><subtitle>Zijie Gao&apos;s Personal Website Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Semi-Automated Elemental Data Extraction from EDX Reports Using Python and ChatGPT</title><link href="https://zj-gao.github.io/blog/2024/XRD/" rel="alternate" type="text/html" title="Semi-Automated Elemental Data Extraction from EDX Reports Using Python and ChatGPT"/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://zj-gao.github.io/blog/2024/XRD</id><content type="html" xml:base="https://zj-gao.github.io/blog/2024/XRD/"><![CDATA[<p><strong>Update 10/4/2024</strong></p> <p> I strongly suspect that the parameters in the ChatGPT model are still changing somehow. The same prompt yields different results even in the same conversation that ever generated perfect results. I also suspect they restrict easy access to the higher-performance OCR model unless you specifically push for it. If you use the prompt I suggested but don’t get the ideal result, start with a single image. Ask it to output the OCR result and then move on to multiple images, and then compile the result into a table. And always, check each image. It already saves you the time and effort of manually logging in, so it's worth spending time to ensure the results are accurate. </p> <p>—————————————–Below is the old Blog—————————————–</p> <p> Do you find logging elemental percentage data from a generated report to be time-consuming? Here's a quick solution, as long as you're familiar with basic Python and ChatGPT prompts: </p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/XRD-480.webp 480w,/assets/img/posts/XRD-800.webp 800w,/assets/img/posts/XRD-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts/XRD.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li><strong>Save all the images from the EDX report</strong> using this script: <a href="https://github.com/ZJ-Gao/OCR_XRD_Reports/blob/main/save_imgs_in_MSword.ipynb"><code class="language-plaintext highlighter-rouge">save_imgs_in_MSword.ipynb</code></a>. Or you can copy from the code snippet at the bottom of this blog. The images will be automatically numbered and saved in the same directory as the report.</li> <li><strong>Remove unnecessary images</strong>: If the report contains images that are not EDX graphs, you can manually delete them.</li> <li><strong>Use the ChatGPT-4 model</strong>: People are cautious about using ChatGPT, fearing it might provide fabricated answers. It’s true, this can happen at times. However, when it comes to OCR usage in EDX reports, I tested it for you. Using the prompts below, the results are reliable. This model has a robust embedded vision system that works well for OCR tasks. I tested several Python OCR libraries, even with data enhancement techniques, but none achieved satisfactory results. If you’re aware of any high-performance open-source OCR models, please share them!</li> <li> <p><strong>Use the following prompt in ChatGPT</strong>:</p> <p><em>For all the images I upload, extract the information from the legend and create a table. The first column should be “Spectrum,” and the subsequent columns should represent elemental information (O, Si, K, Al, Na) and their respective weight percentages (Wt%). Exclude the standard deviation (σ).</em></p> </li> <li><strong>Download the extracted data</strong>: ChatGPT will provide a table with the extracted data, which you can download as a CSV file.</li> <li><strong>Verify the results</strong>: Randomly select a few graphs to check the accuracy of the data extraction by ChatGPT-4.</li> </ol> <h3 id="notes">Notes:</h3> <ul> <li>In ChatGPT, you can upload up to 10 images at a time and are subject to a daily upload limit. This process works well for small tasks.</li> <li>However, for larger datasets (like mine), I’m looking for a solution that can handle batch processing locally. If you have any suggestions or ideas, feel free to reach out!</li> </ul> <h3 id="grab-the-codes">Grab the codes</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">docx</span> <span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="n">io</span>

<span class="c1"># Load the Word document
</span><span class="n">doc_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Camp Century Hawke_USU-4183B 250-355 Points.docx</span><span class="sh">'</span>  <span class="c1"># Update with your document path
</span><span class="n">doc</span> <span class="o">=</span> <span class="nc">Document</span><span class="p">(</span><span class="n">doc_path</span><span class="p">)</span>

<span class="c1"># Iterate through the document and save images
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rel</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">part</span><span class="p">.</span><span class="n">rels</span><span class="p">.</span><span class="nf">values</span><span class="p">()):</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">image</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">rel</span><span class="p">.</span><span class="n">target_ref</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">rel</span><span class="p">.</span><span class="n">target_part</span><span class="p">.</span><span class="n">blob</span>
        <span class="n">image_stream</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="nc">BytesIO</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_stream</span><span class="p">)</span>
        <span class="n">img</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">image_</span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">.png</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># Save images as PNG files
</span>
<span class="c1"># Note: Update the file extension as needed based on the image type
</span></code></pre></div></div>]]></content><author><name></name></author><category term="Data-Science"/><category term="DataScience"/><category term="Chatgpt"/><category term="Automation"/><summary type="html"><![CDATA[Chatgpt 4 is more than a chat robot or graph generator. Its embedded vision model is robust!]]></summary></entry><entry><title type="html">How to use statistical knowledge when analyzing geological data?</title><link href="https://zj-gao.github.io/blog/2024/MLE/" rel="alternate" type="text/html" title="How to use statistical knowledge when analyzing geological data?"/><published>2024-08-13T00:00:00+00:00</published><updated>2024-08-13T00:00:00+00:00</updated><id>https://zj-gao.github.io/blog/2024/MLE</id><content type="html" xml:base="https://zj-gao.github.io/blog/2024/MLE/"><![CDATA[<p>We often learn pure math and statistical concepts in class, encountering jargon like <strong>mean</strong>, <strong>median</strong>, <strong>mode</strong>, <strong>standard deviation</strong>, <strong>variance</strong>, and even more complex terms like <strong>PDF</strong> (Probability Density Function) and <strong>CDF</strong> (Cumulative Distribution Function). Understanding the statistics and the math behind these concepts, which means building a logic linkage between the math, the physical shape of the data distribution, and the statistical meaning behind it is already challenging.</p> <p>When it comes to real life, dealing with our own geological data, it seems we are disconnected from what we learnt from the statistical class. Here I’m going to use my study case as an example to show how I proceed to tie my statistic knowledge and the information I want to read from my data together. The background is that numerous sandy deep-water deposits were recovered by IODP 354, a drilling program in the distal Bengal Fan. I’m curious to know about how thick are these events and how many are they. A histogram is a good way of visualization to inform me the facts I’m about to know. When I try to analyze my data, I don’t necessarily have all that statistical knowledge in the forefront of my mind. Instead, I’m more driven by the geological question I want to answer:</p> <ul> <li>What are the most frequently occurring thicknesses?</li> <li>What about the super thick ones?</li> <li>Will they tail down at the end, or do they represent a peak on their own?</li> </ul> <p>I first plotted a histogram under a linear scale, but there wasn’t much information to be gleaned from it. Due to my past experience, I knew that geological data often benefits from being plotted on a logarithmic scale, so I tried that as well. After a few attempts at adjusting the bin numbers, the histogram finally looked both informative and aesthetically pleasing.</p> <h3 id="observations-from-the-data"><strong>Observations from the Data</strong></h3> <p>As I began my observations (The final graph plotted with my data will be attached here once it is published. For now, the graph’s shape is similar to the thumbnail image when you clicked the post):</p> <ul> <li>The distribution looked skewed to the left.</li> <li>The most frequently occurring thicknesses seemed to be those between 10 cm and 20 cm.</li> <li>However, I noticed that if I changed the bin number, the exact range of the most frequent group would slightly shift.</li> <li>Additionally, I observed that the super thick events, those over 100 cm, were situated at the right tail of the distribution.</li> </ul> <p>These are good observations, based on intuition, but to ensure that the information is consistently conveyed to others, we need to quantify our observations. After all, different people may have different definitions for what qualifies as “super thick,” a term not globally defined.</p> <h3 id="introducing-statistical-concepts"><strong>Introducing Statistical Concepts</strong></h3> <p>This is where statistical knowledge becomes invaluable. The bin number is a parameter you can adjust to better visualize the data—it’s flexible and helps you see different aspects of the dataset. However, the <strong>descriptive statistics</strong> are fixed; they are inherent properties of the dataset.</p> <ul> <li><strong>Mode</strong>: The mode is the parameter used to describe the most frequently occurring value. It can be calculated as the value that appears most often in the dataset.</li> <li><strong>Outliers</strong>: The super thick sediment gravity flows (SGFs) at the tail are outliers. To define outliers formally, one common method is using <strong>mean + 2\(\sigma\) (standard deviations)</strong>. In this case, for our data, this threshold is around 2 meters. Rather than calling these events “super thick,” we define them as <strong>outsized</strong> to make the term more formal.</li> </ul> <h3 id="understanding-the-empirical-rule"><strong>Understanding the Empirical Rule</strong></h3> <p>One thing to keep in mind is the <strong>empirical rule</strong>, also known as the 68-95-99.7 rule. This rule states that:</p> <ul> <li>68% of the data falls within one standard deviation of the mean,</li> <li>95% falls within two standard deviations,</li> <li>99.7% falls within three standard deviations.</li> </ul> <p>However, this rule is derived from a <strong>symmetric normal distribution</strong> and does not apply to skewed data, like the left-skewed distribution we’re dealing with here.</p> <h3 id="quantifying-the-observations"><strong>Quantifying the Observations</strong></h3> <p>Now, with these quantifying ideas in mind, let’s revisit the graph. We still use descriptive terms like <strong>left-skewed lognormal distribution</strong>, but now we supplement these with numbers, like the mode and the threshold for outliers. What if we want to delve deeper, or just say we want to be a little more nerdier mathematically as a geologist:</p> <ul> <li><strong>Fitting a Distribution</strong>: How can we use a solid equation to fit this distribution?</li> <li><strong>Calculating Probabilities</strong>: What’s the probability of forming these outsized SGFs?</li> </ul> <h3 id="fitting-the-lognormal-distribution"><strong>Fitting the Lognormal Distribution</strong></h3> <p>We’ve already determined that the distribution pattern we’re dealing with is a <strong>lognormal distribution</strong>. The general equation for the PDF of a lognormal distribution is:</p> \[f(x) = \frac{1}{x\sigma\sqrt{2\pi}} \exp\left(-\frac{(\ln(x) - \mu)^2}{2\sigma^2}\right)\] <p>Where:</p> <ul> <li>\(x\) is the thickness,</li> <li>\(\mu\) is the mean of the log-transformed data,</li> <li>\(\sigma\) is the standard deviation of the log-transformed data.</li> </ul> <h3 id="customizing-the-distribution-using-maximum-likelihood-estimation-mle"><strong>Customizing the Distribution Using Maximum Likelihood Estimation (MLE)</strong></h3> <p>To apply the lognormal distribution equation to our data, we need to <strong>personalize</strong> it by estimating the parameters \(\mu\) and \(\sigma\) using the <strong>method of Maximum Likelihood Estimation (MLE)</strong>. Here’s how you can execute MLE with logarithms base 10:</p> <ol> <li><strong>Log-transform the data (using log base 10)</strong>: <ul> <li>First, transform your thickness data by taking the logarithm base 10 of each data point, resulting in a new dataset \(\log_{10}(x)\).</li> </ul> </li> <li><strong>Write the likelihood function</strong>: <ul> <li>The likelihood function is the <strong>product of the PDF</strong> for each data point in your sample because this is how you <strong>mathematically combine independent observations</strong> to reflect <strong>the overall likelihood of the entire dataset</strong>. In essence, it aggregates the information from all the individual data points to provide a <strong>comprehensive measure</strong> of how likely the observed data is under the given distribution parameters.</li> <li>The likelihood function represents the probability of observing your data given specific values of the parameters \(\mu\) and \(\sigma\). For a lognormal distribution with base 10 logarithms, the likelihood function \(L(\mu, \sigma)\) based on the transformed data can be written as:</li> </ul> \[L(\mu, \sigma) = \prod_{i=1}^{n} \frac{1}{x_i \sigma \sqrt{2\pi}} \exp \left( -\frac{(\log_{10}(x_i) - \mu)^2}{2\sigma^2} \right)\] </li> <li><strong>Derive the log-likelihood function</strong>: <ul> <li>To simplify the likelihood function, take the logarithm base 10 of the likelihood function, which converts the product into a sum. This gives you the <strong>log-likelihood function</strong>:</li> </ul> \[\log_{10}(L(\mu, \sigma)) = -\frac{n}{2} \log_{10}(2\pi) - n \log_{10}(\sigma) - \sum_{i=1}^{n} \log_{10}(x_i) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (\log_{10}(x_i) - \mu)^2\] </li> <li> <p><strong>Differentiate the log-likelihood function</strong>:</p> <p>The next step is to maximize the log-likelihood function with respect to \(\mu\) and \(\sigma\) to find the values that make the observed data most probable. The essence is to calculate the derivatives of the \(\log_{10}(L(\mu, \sigma))\) with respect to \(\mu\) and \(\sigma\), and find the maximum likelihood estimates for these parameters.</p> <p>To find the maximum likelihood estimates for \(\mu\) and \(\sigma\), we differentiate the log-likelihood function with respect to each parameter.</p> <ul> <li><strong>For</strong> \(\mu\): Differentiate the log-likelihood function with respect to \(\mu\):</li> </ul> \[\frac{\partial \log_{10}(L(\mu, \sigma))}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^{n} (\log_{10}(x_i) - \mu)\] <ul> <li><strong>For</strong> \(\sigma\): Differentiate the log-likelihood function with respect to \(\sigma\):</li> </ul> \[\frac{\partial \log_{10}(L(\mu, \sigma))}{\partial \sigma} = -\frac{n}{\sigma \ln(10)} + \frac{1}{\sigma^3 \ln(10)} \sum_{i=1}^{n} (\log_{10}(x_i) - \mu)^2\] </li> <li> <p><strong>Set the derivatives to zero to find the maximum</strong>:</p> <p>To find the maximum of the log-likelihood function, set the partial derivatives with respect to \(\mu\) and \(\sigma\) equal to zero:</p> <ul> <li><strong>For</strong> \(\mu\):</li> </ul> \[\frac{\partial \log_{10}(L(\mu, \sigma))}{\partial \mu} = 0 \implies \sum_{i=1}^{n} (\log_{10}(x_i) - \mu) = 0\] <p>This equation simplifies to:</p> \[\mu = \frac{1}{n} \sum_{i=1}^{n} \log_{10}(x_i)\] <p>After applying the dataset, each thickness data point corresponds to \(x_i\). \(\mu\) will be obtained, which is the mean of the log-transformed data.</p> <p>The same workflow applies for obtaining \(\sigma\).</p> <ul> <li><strong>For</strong> \(\sigma\):</li> </ul> \[\frac{\partial \log_{10}(L(\mu, \sigma))}{\partial \sigma} = 0 \implies -\frac{n}{\sigma \ln(10)} + \frac{1}{\sigma^3 \ln(10)} \sum_{i=1}^{n} (\log_{10}(x_i) - \mu)^2 = 0\] <p>This equation simplifies to:</p> \[\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (\log_{10}(x_i) - \mu)^2\] <p>\(\sigma\) can then be calculated after applying the dataset, which is the standard deviation of the log-transformed data.</p> <p>So far, the values of \(\mu\) and \(\sigma\) that maximize the log-likelihood function are your MLE estimates. These estimates will be used to define your personalized lognormal distribution, and to construct the probability density function (PDF) and cumulative distribution function (CDF) specific to your data, enabling further analysis and interpretation.</p> <p>In high school, teachers just teach you the equation for calculating \(\mu\) and \(\sigma\), and let you memorize them for exams. Now packages in Python like <code class="language-plaintext highlighter-rouge">pandas</code> can give you all the descriptive stats with just one line of code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">df</span><span class="p">.</span><span class="nf">describe</span><span class="p">()</span>
</code></pre></div> </div> <p>But if you take the time to work through these calculations yourself, it really helps you understand why we’re doing them and what they actually mean. Plus, there’s a sense of accomplishment when your calculated results align with what you learned from your high school teacher. We are standing on the shoulders of giants.</p> </li> </ol> <h3 id="calculating-the-cumulative-distribution-function-cdf"><strong>Calculating the Cumulative Distribution Function (CDF)</strong></h3> <p>Once we have the equation for the PDF, we can calculate the <strong>Cumulative Distribution Function (CDF)</strong>, which represents the accumulating probability, by integrating the PDF:</p> \[F(x) = \int_{-\infty}^{x} f(t) \, dt\] \[F(x) = P(X \leq x) = \Phi\left(\frac{\log_{10}(x) - \mu}{\sigma}\right)\] <p>This CDF will give us the cumulative probability up to a certain thickness.</p> <h3 id="determining-the-probability-of-outsized-sgfs"><strong>Determining the Probability of Outsized SGFs</strong></h3> <p>To calculate the probability of forming outsized SGFs with thicknesses between 2m and 10m, we follow these steps:</p> <ol> <li><strong>Calculate the CDF for 2m and 10m</strong>: Find the CDF values for these thicknesses using the personalized lognormal distribution.</li> <li><strong>Subtract the CDF values</strong>: Subtract the CDF value at 2m from the CDF value at 10m to get the probability of a thickness falling within this range.</li> </ol> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>In this blog, we’ve explored how statistical tools can be applied to geological data, making those initially confusing statistical terms our allies. With these quantified observations and a clear understanding of their statistical meaning, we can confidently move forward with our geological interpretations, using data to back up our insights.</p>]]></content><author><name></name></author><category term="Data-Science"/><category term="DataScience"/><category term="Math"/><category term="Statistics"/><summary type="html"><![CDATA[Bridging the Gap Between Statistics and Geological Data\(:\) A Little Case Using Histogram. Will update with data and graphs once published!]]></summary></entry></feed>